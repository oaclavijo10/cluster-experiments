{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"cluster_experiments \u00b6 A library to run power analysis and analyse clustered and switchback experiments. Example \u00b6 This is a comprehensive example of how to use this library. There are simpler ways to run power analysis but this shows all the building blocks of the library. from datetime import date import numpy as np import pandas as pd from cluster_experiments.experiment_analysis import GeeExperimentAnalysis from cluster_experiments.perturbator import UniformPerturbator from cluster_experiments.power_analysis import PowerAnalysis from cluster_experiments.random_splitter import SwitchbackSplitter # Create fake data N = 1_000 clusters = [ f \"Cluster { i } \" for i in range ( 100 )] dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 1 , 32 )] df = pd . DataFrame ( { \"cluster\" : np . random . choice ( clusters , size = N ), \"target\" : np . random . normal ( 0 , 1 , size = N ), \"date\" : np . random . choice ( dates , size = N ), } ) # A switchback experiment is going to be run, prepare the switchback splitter for the analysis sw = SwitchbackSplitter ( clusters = clusters , dates = dates , ) # We use a uniform perturbator to add artificial effect on the treated on the power analysis perturbator = UniformPerturbator ( average_effect = 0.1 , ) # Use gee to run the analysis analysis = GeeExperimentAnalysis ( cluster_cols = [ \"cluster\" , \"date\" ], ) # Run the power analysis pw = PowerAnalysis ( perturbator = perturbator , splitter = sw , analysis = analysis , n_simulations = 50 ) power = pw . power_analysis ( df ) print ( f \" { power = } \" ) Features \u00b6 The library offers the following classes: Regarding power analysis: PowerAnalysis : to run power analysis on a clustered/switchback design UniformPerturbator : to artificially perturb treated group with uniform perturbations BinaryPerturbator : to artificially perturb treated group for binary outcomes Regarding splitting data: ClusteredSplitter : to split data based on clusters SwitchbackSplitter : to split data based using switchback method BalancedClusteredSplitter : to split data based on clusters in a balanced way BalancedSwitchbackSplitter : to split data based using switchback method in a balanced way Regarding analysis: GeeExperimentAnalysis : to run GEE analysis on a the results of a clustered design TargetAggregation : to add pre-experimental data of the outcome to reduce variance Other: PowerConfig : to conviently configure PowerAnalysis class Installation \u00b6 You can install this package via pip . pip install cluster-experiments It may be safer to install via; python -m pip install cluster-experiments Contributing \u00b6 git clone git @github . com : david26694 / cluster - experiments . git cd cluster - experiments make install - dev","title":"Index"},{"location":"index.html#cluster_experiments","text":"A library to run power analysis and analyse clustered and switchback experiments.","title":"cluster_experiments"},{"location":"index.html#example","text":"This is a comprehensive example of how to use this library. There are simpler ways to run power analysis but this shows all the building blocks of the library. from datetime import date import numpy as np import pandas as pd from cluster_experiments.experiment_analysis import GeeExperimentAnalysis from cluster_experiments.perturbator import UniformPerturbator from cluster_experiments.power_analysis import PowerAnalysis from cluster_experiments.random_splitter import SwitchbackSplitter # Create fake data N = 1_000 clusters = [ f \"Cluster { i } \" for i in range ( 100 )] dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 1 , 32 )] df = pd . DataFrame ( { \"cluster\" : np . random . choice ( clusters , size = N ), \"target\" : np . random . normal ( 0 , 1 , size = N ), \"date\" : np . random . choice ( dates , size = N ), } ) # A switchback experiment is going to be run, prepare the switchback splitter for the analysis sw = SwitchbackSplitter ( clusters = clusters , dates = dates , ) # We use a uniform perturbator to add artificial effect on the treated on the power analysis perturbator = UniformPerturbator ( average_effect = 0.1 , ) # Use gee to run the analysis analysis = GeeExperimentAnalysis ( cluster_cols = [ \"cluster\" , \"date\" ], ) # Run the power analysis pw = PowerAnalysis ( perturbator = perturbator , splitter = sw , analysis = analysis , n_simulations = 50 ) power = pw . power_analysis ( df ) print ( f \" { power = } \" )","title":"Example"},{"location":"index.html#features","text":"The library offers the following classes: Regarding power analysis: PowerAnalysis : to run power analysis on a clustered/switchback design UniformPerturbator : to artificially perturb treated group with uniform perturbations BinaryPerturbator : to artificially perturb treated group for binary outcomes Regarding splitting data: ClusteredSplitter : to split data based on clusters SwitchbackSplitter : to split data based using switchback method BalancedClusteredSplitter : to split data based on clusters in a balanced way BalancedSwitchbackSplitter : to split data based using switchback method in a balanced way Regarding analysis: GeeExperimentAnalysis : to run GEE analysis on a the results of a clustered design TargetAggregation : to add pre-experimental data of the outcome to reduce variance Other: PowerConfig : to conviently configure PowerAnalysis class","title":"Features"},{"location":"index.html#installation","text":"You can install this package via pip . pip install cluster-experiments It may be safer to install via; python -m pip install cluster-experiments","title":"Installation"},{"location":"index.html#contributing","text":"git clone git @github . com : david26694 / cluster - experiments . git cd cluster - experiments make install - dev","title":"Contributing"},{"location":"api/cupac_model.html","text":"from cluster_experiments.cupac import * \u00b6 EmptyRegressor \u00b6 Empty regressor class. It does not do anything, used to glue the code of other estimators and PowerAnalysis Each Regressor should have: - fit method: Uses pre experiment data to fit some kind of model to be used as a covariate and reduce variance. - predict method: Uses the fitted model to add the covariate on the experiment data. It can add aggregates of the target in older data as a covariate, or a model (cupac) to predict the target. TargetAggregation \u00b6 Adds average of target using pre-experiment data Parameters Name Type Description Default agg_col str Column to group by to aggregate target required target_col str Column to aggregate 'target' smoothing_factor int Smoothing factor for the smoothed mean 20 Usage: import pandas as pd from cluster_experiments.cupac import TargetAggregation df = pd . DataFrame ({ \"agg_col\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"c\" , \"c\" ], \"target_col\" : [ 1 , 2 , 3 , 4 , 5 , 6 ]}) new_df = pd . DataFrame ({ \"agg_col\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"c\" , \"c\" ]}) target_agg = TargetAggregation ( \"agg_col\" , \"target_col\" ) target_agg . fit ( df ) df_with_target_agg = target_agg . predict ( new_df ) print ( df_with_target_agg ) fit ( self , X , y ) \u00b6 Show source code in cluster_experiments/cupac.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def fit ( self , X : pd . DataFrame , y : pd . Series ) -> \"TargetAggregation\" : \"\"\"Fits \"target encoder\" model to pre-experiment data\"\"\" pre_experiment_df = X . copy () pre_experiment_df [ self . target_col ] = y self . pre_experiment_mean = pre_experiment_df [ self . target_col ] . mean () self . pre_experiment_agg_df = ( pre_experiment_df . assign ( count = 1 ) . groupby ( self . agg_col , as_index = False ) . agg ({ self . target_col : \"sum\" , \"count\" : \"sum\" }) . assign ( ** { self . mean_target_col : lambda x : x [ self . target_col ] / x [ \"count\" ], self . smooth_mean_target_col : lambda x : ( x [ self . target_col ] + self . smoothing_factor * self . pre_experiment_mean ) / ( x [ \"count\" ] + self . smoothing_factor ), } ) . drop ( columns = [ \"count\" , self . target_col ]) ) return self Fits \"target encoder\" model to pre-experiment data from_config ( config ) (classmethod) \u00b6 Show source code in cluster_experiments/cupac.py 95 96 97 98 99 100 101 102 @classmethod def from_config ( cls , config ): \"\"\"Creates TargetAggregation from PowerConfig\"\"\" return cls ( agg_col = config . agg_col , target_col = config . target_col , smoothing_factor = config . smoothing_factor , ) Creates TargetAggregation from PowerConfig predict ( self , X ) \u00b6 Show source code in cluster_experiments/cupac.py 85 86 87 88 89 90 91 92 93 def predict ( self , X : pd . DataFrame ) -> ArrayLike : \"\"\"Adds average target of pre-experiment data to experiment data\"\"\" return ( X . merge ( self . pre_experiment_agg_df , how = \"left\" , on = self . agg_col )[ self . smooth_mean_target_col ] . fillna ( self . pre_experiment_mean ) . values ) Adds average target of pre-experiment data to experiment data","title":"Pre experiment outcome model"},{"location":"api/cupac_model.html#from-cluster_experimentscupac-import","text":"","title":"from cluster_experiments.cupac import *"},{"location":"api/cupac_model.html#cluster_experiments.cupac.EmptyRegressor","text":"Empty regressor class. It does not do anything, used to glue the code of other estimators and PowerAnalysis Each Regressor should have: - fit method: Uses pre experiment data to fit some kind of model to be used as a covariate and reduce variance. - predict method: Uses the fitted model to add the covariate on the experiment data. It can add aggregates of the target in older data as a covariate, or a model (cupac) to predict the target.","title":"EmptyRegressor"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation","text":"Adds average of target using pre-experiment data Parameters Name Type Description Default agg_col str Column to group by to aggregate target required target_col str Column to aggregate 'target' smoothing_factor int Smoothing factor for the smoothed mean 20 Usage: import pandas as pd from cluster_experiments.cupac import TargetAggregation df = pd . DataFrame ({ \"agg_col\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"c\" , \"c\" ], \"target_col\" : [ 1 , 2 , 3 , 4 , 5 , 6 ]}) new_df = pd . DataFrame ({ \"agg_col\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"c\" , \"c\" ]}) target_agg = TargetAggregation ( \"agg_col\" , \"target_col\" ) target_agg . fit ( df ) df_with_target_agg = target_agg . predict ( new_df ) print ( df_with_target_agg )","title":"TargetAggregation"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.fit","text":"Show source code in cluster_experiments/cupac.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def fit ( self , X : pd . DataFrame , y : pd . Series ) -> \"TargetAggregation\" : \"\"\"Fits \"target encoder\" model to pre-experiment data\"\"\" pre_experiment_df = X . copy () pre_experiment_df [ self . target_col ] = y self . pre_experiment_mean = pre_experiment_df [ self . target_col ] . mean () self . pre_experiment_agg_df = ( pre_experiment_df . assign ( count = 1 ) . groupby ( self . agg_col , as_index = False ) . agg ({ self . target_col : \"sum\" , \"count\" : \"sum\" }) . assign ( ** { self . mean_target_col : lambda x : x [ self . target_col ] / x [ \"count\" ], self . smooth_mean_target_col : lambda x : ( x [ self . target_col ] + self . smoothing_factor * self . pre_experiment_mean ) / ( x [ \"count\" ] + self . smoothing_factor ), } ) . drop ( columns = [ \"count\" , self . target_col ]) ) return self Fits \"target encoder\" model to pre-experiment data","title":"fit()"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.from_config","text":"Show source code in cluster_experiments/cupac.py 95 96 97 98 99 100 101 102 @classmethod def from_config ( cls , config ): \"\"\"Creates TargetAggregation from PowerConfig\"\"\" return cls ( agg_col = config . agg_col , target_col = config . target_col , smoothing_factor = config . smoothing_factor , ) Creates TargetAggregation from PowerConfig","title":"from_config()"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.predict","text":"Show source code in cluster_experiments/cupac.py 85 86 87 88 89 90 91 92 93 def predict ( self , X : pd . DataFrame ) -> ArrayLike : \"\"\"Adds average target of pre-experiment data to experiment data\"\"\" return ( X . merge ( self . pre_experiment_agg_df , how = \"left\" , on = self . agg_col )[ self . smooth_mean_target_col ] . fillna ( self . pre_experiment_mean ) . values ) Adds average target of pre-experiment data to experiment data","title":"predict()"},{"location":"api/experiment_analysis.html","text":"from cluster_experiments.experiment_analysis import * \u00b6 ExperimentAnalysis \u00b6 Abstract class to run the analysis of a given experiment In order to create your own ExperimentAnalysis, you should create a derived class that implements the analysis_pvalue method. It can also be used as a component of the PowerAnalysis class. Parameters Name Type Description Default cluster_cols List[str] list of columns to use as clusters required target_col str name of the column containing the variable to measure 'target' treatment_col str name of the column containing the treatment variable 'treatment' treatment str name of the treatment to use as the treated group 'B' covariates Optional[List[str]] list of columns to use as covariates None analysis_pvalue ( self , df ) \u00b6 Show source code in cluster_experiments/experiment_analysis.py 51 52 53 54 55 56 57 @abstractmethod def analysis_pvalue ( self , df : pd . DataFrame , ) -> float : \"\"\"Returns the p-value of the analysis. Expects treatment to be 0-1 variable\"\"\" pass Returns the p-value of the analysis. Expects treatment to be 0-1 variable from_config ( config ) (classmethod) \u00b6 Show source code in cluster_experiments/experiment_analysis.py 69 70 71 72 73 74 75 76 77 78 @classmethod def from_config ( cls , config ): \"\"\"Creates an ExperimentAnalysis object from a PowerConfig object\"\"\" return cls ( cluster_cols = config . cluster_cols , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , covariates = config . covariates , ) Creates an ExperimentAnalysis object from a PowerConfig object get_pvalue ( self , df ) \u00b6 Show source code in cluster_experiments/experiment_analysis.py 59 60 61 62 63 64 65 66 67 def get_pvalue ( self , df : pd . DataFrame ) -> float : \"\"\"Returns the p-value of the analysis Arguments: df: dataframe containing the data to analyze \"\"\" df = df . copy () df = self . _create_binary_treatment ( df ) return self . analysis_pvalue ( df ) Returns the p-value of the analysis Parameters Name Type Description Default df DataFrame dataframe containing the data to analyze required GeeExperimentAnalysis \u00b6 Class to run GEE analysis Usage: from cluster_experiments.experiment_analysis import GeeExperimentAnalysis import pandas as pd df = pd . DataFrame ({ 'x' : [ 1 , 2 , 3 , 0 , 0 , 1 ], 'treatment' : [ \"A\" ] * 3 + [ \"B\" ] * 3 , 'cluster' : [ 1 ] * 6 , }) GeeExperimentAnalysis ( cluster_cols = [ 'cluster' ], target_col = 'x' , ) . get_pvalue ( df ) analysis_pvalue ( self , df ) \u00b6 Show source code in cluster_experiments/experiment_analysis.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def analysis_pvalue ( self , df : pd . DataFrame ) -> float : \"\"\"Returns the p-value of the analysis Arguments: df: dataframe containing the data to analyze \"\"\" results_gee = sm . GEE . from_formula ( self . formula , data = df , groups = self . _get_cluster_column ( df ), family = self . fam , cov_struct = self . va , ) . fit () return results_gee . pvalues [ self . treatment_col ] Returns the p-value of the analysis Parameters Name Type Description Default df DataFrame dataframe containing the data to analyze required","title":"Experiment analysis"},{"location":"api/experiment_analysis.html#from-cluster_experimentsexperiment_analysis-import","text":"","title":"from cluster_experiments.experiment_analysis import *"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis","text":"Abstract class to run the analysis of a given experiment In order to create your own ExperimentAnalysis, you should create a derived class that implements the analysis_pvalue method. It can also be used as a component of the PowerAnalysis class. Parameters Name Type Description Default cluster_cols List[str] list of columns to use as clusters required target_col str name of the column containing the variable to measure 'target' treatment_col str name of the column containing the treatment variable 'treatment' treatment str name of the treatment to use as the treated group 'B' covariates Optional[List[str]] list of columns to use as covariates None","title":"ExperimentAnalysis"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_pvalue","text":"Show source code in cluster_experiments/experiment_analysis.py 51 52 53 54 55 56 57 @abstractmethod def analysis_pvalue ( self , df : pd . DataFrame , ) -> float : \"\"\"Returns the p-value of the analysis. Expects treatment to be 0-1 variable\"\"\" pass Returns the p-value of the analysis. Expects treatment to be 0-1 variable","title":"analysis_pvalue()"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.from_config","text":"Show source code in cluster_experiments/experiment_analysis.py 69 70 71 72 73 74 75 76 77 78 @classmethod def from_config ( cls , config ): \"\"\"Creates an ExperimentAnalysis object from a PowerConfig object\"\"\" return cls ( cluster_cols = config . cluster_cols , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , covariates = config . covariates , ) Creates an ExperimentAnalysis object from a PowerConfig object","title":"from_config()"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_pvalue","text":"Show source code in cluster_experiments/experiment_analysis.py 59 60 61 62 63 64 65 66 67 def get_pvalue ( self , df : pd . DataFrame ) -> float : \"\"\"Returns the p-value of the analysis Arguments: df: dataframe containing the data to analyze \"\"\" df = df . copy () df = self . _create_binary_treatment ( df ) return self . analysis_pvalue ( df ) Returns the p-value of the analysis Parameters Name Type Description Default df DataFrame dataframe containing the data to analyze required","title":"get_pvalue()"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis","text":"Class to run GEE analysis Usage: from cluster_experiments.experiment_analysis import GeeExperimentAnalysis import pandas as pd df = pd . DataFrame ({ 'x' : [ 1 , 2 , 3 , 0 , 0 , 1 ], 'treatment' : [ \"A\" ] * 3 + [ \"B\" ] * 3 , 'cluster' : [ 1 ] * 6 , }) GeeExperimentAnalysis ( cluster_cols = [ 'cluster' ], target_col = 'x' , ) . get_pvalue ( df )","title":"GeeExperimentAnalysis"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_pvalue","text":"Show source code in cluster_experiments/experiment_analysis.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def analysis_pvalue ( self , df : pd . DataFrame ) -> float : \"\"\"Returns the p-value of the analysis Arguments: df: dataframe containing the data to analyze \"\"\" results_gee = sm . GEE . from_formula ( self . formula , data = df , groups = self . _get_cluster_column ( df ), family = self . fam , cov_struct = self . va , ) . fit () return results_gee . pvalues [ self . treatment_col ] Returns the p-value of the analysis Parameters Name Type Description Default df DataFrame dataframe containing the data to analyze required","title":"analysis_pvalue()"},{"location":"api/perturbator.html","text":"from cluster_experiments.perturbator import * \u00b6 BinaryPerturbator \u00b6 BinaryPerturbator is a Perturbator that adds is used to deal with binary outcome variables. It randomly selects some treated instances and flips their outcome from 0 to 1 or 1 to 0, depending on the effect being positive or negative perturbate ( self , df ) \u00b6 Show source code in cluster_experiments/perturbator.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Usage: ```python from cluster_experiments.perturbator import BinaryPerturbator import pandas as pd df = pd.DataFrame({\"target\": [1, 0, 1], \"treatment\": [\"A\", \"B\", \"A\"]}) perturbator = BinaryPerturbator(average_effect=0.1) perturbator.perturbate(df) ``` \"\"\" df = df . copy () . reset_index ( drop = True ) from_target , to_target = 1 , 0 if self . average_effect > 0 : from_target , to_target = 0 , 1 n_transformed = abs ( int ( self . average_effect * len ( df . query ( self . treated_query ))) ) idx = list ( # Sample of negative cases in group B df . query ( f \" { self . target_col } == { from_target } & { self . treated_query } \" ) . pipe ( self . _sample_max , n = n_transformed ) . index . drop_duplicates () ) df . loc [ idx , self . target_col ] = to_target return df Usage: from cluster_experiments.perturbator import BinaryPerturbator import pandas as pd df = pd . DataFrame ({ \"target\" : [ 1 , 0 , 1 ], \"treatment\" : [ \"A\" , \"B\" , \"A\" ]}) perturbator = BinaryPerturbator ( average_effect = 0.1 ) perturbator . perturbate ( df ) Perturbator \u00b6 Abstract perturbator. Perturbators are used to simulate a fictitious effect when running a power analysis. The idea is that, when running a power analysis, we split our instances according to a RandomSplitter, and the instances that got the treatment, are perturbated with a fictional effect via the Perturbator. In order to create your own perturbator, you should create a derived class that implements the perturbate method. The perturbate method should add the average effect in the desired way and return the dataframe with the extra average effect, without affecting the initial dataframe. Keep in mind to use df = df.copy() in the first line of the perturbate method. Parameters Name Type Description Default average_effect float The average effect of the treatment required treatment str name of the treatment to use as the treated group 'B' treatment_col str The name of the column that contains the treatment 'treatment' treatment str name of the treatment to use as the treated group 'B' from_config ( config ) (classmethod) \u00b6 Show source code in cluster_experiments/perturbator.py 43 44 45 46 47 48 49 50 51 @classmethod def from_config ( cls , config ): \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\" return cls ( average_effect = config . average_effect , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , ) Creates a Perturbator object from a PowerConfig object perturbate ( self , df ) \u00b6 Show source code in cluster_experiments/perturbator.py 38 39 40 41 @abstractmethod def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to perturbate a dataframe\"\"\" pass Method to perturbate a dataframe UniformPerturbator \u00b6 UniformPerturbator is a Perturbator that adds a uniform effect to the target column of the treated instances. perturbate ( self , df ) \u00b6 Show source code in cluster_experiments/perturbator.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Usage: ```python from cluster_experiments.perturbator import UniformPerturbator import pandas as pd df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]}) perturbator = UniformPerturbator(average_effect=1) perturbator.perturbate(df) ``` \"\"\" df = df . copy () . reset_index ( drop = True ) df . loc [ df [ self . treatment_col ] == self . treatment , self . target_col ] += self . average_effect return df Usage: from cluster_experiments.perturbator import UniformPerturbator import pandas as pd df = pd . DataFrame ({ \"target\" : [ 1 , 2 , 3 ], \"treatment\" : [ \"A\" , \"B\" , \"A\" ]}) perturbator = UniformPerturbator ( average_effect = 1 ) perturbator . perturbate ( df )","title":"Perturbators"},{"location":"api/perturbator.html#from-cluster_experimentsperturbator-import","text":"","title":"from cluster_experiments.perturbator import *"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator","text":"BinaryPerturbator is a Perturbator that adds is used to deal with binary outcome variables. It randomly selects some treated instances and flips their outcome from 0 to 1 or 1 to 0, depending on the effect being positive or negative","title":"BinaryPerturbator"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator.perturbate","text":"Show source code in cluster_experiments/perturbator.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Usage: ```python from cluster_experiments.perturbator import BinaryPerturbator import pandas as pd df = pd.DataFrame({\"target\": [1, 0, 1], \"treatment\": [\"A\", \"B\", \"A\"]}) perturbator = BinaryPerturbator(average_effect=0.1) perturbator.perturbate(df) ``` \"\"\" df = df . copy () . reset_index ( drop = True ) from_target , to_target = 1 , 0 if self . average_effect > 0 : from_target , to_target = 0 , 1 n_transformed = abs ( int ( self . average_effect * len ( df . query ( self . treated_query ))) ) idx = list ( # Sample of negative cases in group B df . query ( f \" { self . target_col } == { from_target } & { self . treated_query } \" ) . pipe ( self . _sample_max , n = n_transformed ) . index . drop_duplicates () ) df . loc [ idx , self . target_col ] = to_target return df Usage: from cluster_experiments.perturbator import BinaryPerturbator import pandas as pd df = pd . DataFrame ({ \"target\" : [ 1 , 0 , 1 ], \"treatment\" : [ \"A\" , \"B\" , \"A\" ]}) perturbator = BinaryPerturbator ( average_effect = 0.1 ) perturbator . perturbate ( df )","title":"perturbate()"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator","text":"Abstract perturbator. Perturbators are used to simulate a fictitious effect when running a power analysis. The idea is that, when running a power analysis, we split our instances according to a RandomSplitter, and the instances that got the treatment, are perturbated with a fictional effect via the Perturbator. In order to create your own perturbator, you should create a derived class that implements the perturbate method. The perturbate method should add the average effect in the desired way and return the dataframe with the extra average effect, without affecting the initial dataframe. Keep in mind to use df = df.copy() in the first line of the perturbate method. Parameters Name Type Description Default average_effect float The average effect of the treatment required treatment str name of the treatment to use as the treated group 'B' treatment_col str The name of the column that contains the treatment 'treatment' treatment str name of the treatment to use as the treated group 'B'","title":"Perturbator"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator.from_config","text":"Show source code in cluster_experiments/perturbator.py 43 44 45 46 47 48 49 50 51 @classmethod def from_config ( cls , config ): \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\" return cls ( average_effect = config . average_effect , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , ) Creates a Perturbator object from a PowerConfig object","title":"from_config()"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator.perturbate","text":"Show source code in cluster_experiments/perturbator.py 38 39 40 41 @abstractmethod def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to perturbate a dataframe\"\"\" pass Method to perturbate a dataframe","title":"perturbate()"},{"location":"api/perturbator.html#cluster_experiments.perturbator.UniformPerturbator","text":"UniformPerturbator is a Perturbator that adds a uniform effect to the target column of the treated instances.","title":"UniformPerturbator"},{"location":"api/perturbator.html#cluster_experiments.perturbator.UniformPerturbator.perturbate","text":"Show source code in cluster_experiments/perturbator.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def perturbate ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Usage: ```python from cluster_experiments.perturbator import UniformPerturbator import pandas as pd df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]}) perturbator = UniformPerturbator(average_effect=1) perturbator.perturbate(df) ``` \"\"\" df = df . copy () . reset_index ( drop = True ) df . loc [ df [ self . treatment_col ] == self . treatment , self . target_col ] += self . average_effect return df Usage: from cluster_experiments.perturbator import UniformPerturbator import pandas as pd df = pd . DataFrame ({ \"target\" : [ 1 , 2 , 3 ], \"treatment\" : [ \"A\" , \"B\" , \"A\" ]}) perturbator = UniformPerturbator ( average_effect = 1 ) perturbator . perturbate ( df )","title":"perturbate()"},{"location":"api/power_analysis.html","text":"from cluster_experiments.power_analysis import * \u00b6 PowerAnalysis \u00b6 Class used to run Power analysis. It does so by running simulations. In each simulation: 1. Assign treatment to dataframe randomly 2. Perturbate dataframe 3. Add pre-experiment data if needed 4. Run analysis Finally it returns the power of the analysis by counting how many times the effect was detected. Parameters Name Type Description Default perturbator Perturbator Perturbator class to perturbate dataframe with treatment assigned. required splitter RandomSplitter RandomSplitter class to randomly assign treatment to dataframe. required analysis ExperimentAnalysis ExperimentAnalysis class to use for analysis. required cupac_model Optional[sklearn.base.BaseEstimator] Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added. None target_col str Name of the column with the outcome variable. 'target' treatment_col str Name of the column with the treatment variable. 'treatment' treatment str value of treatment_col considered to be treatment (not control) 'B' n_simulations int Number of simulations to run. 100 alpha float Significance level. 0.05 features_cupac_model Optional[List[str]] Covariates to be used in cupac model None Usage: from datetime import date import numpy as np import pandas as pd from cluster_experiments.experiment_analysis import GeeExperimentAnalysis from cluster_experiments.perturbator import UniformPerturbator from cluster_experiments.power_analysis import PowerAnalysis from cluster_experiments.random_splitter import SwitchbackSplitter N = 1_000 users = [ f \"User { i } \" for i in range ( 1000 )] clusters = [ f \"Cluster { i } \" for i in range ( 100 )] dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 1 , 32 )] df = pd . DataFrame ( { \"cluster\" : np . random . choice ( clusters , size = N ), \"target\" : np . random . normal ( 0 , 1 , size = N ), \"user\" : np . random . choice ( users , size = N ), \"date\" : np . random . choice ( dates , size = N ), } ) experiment_dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 15 , 32 )] sw = SwitchbackSplitter ( clusters = clusters , dates = experiment_dates , ) perturbator = UniformPerturbator ( average_effect = 0.1 , ) analysis = GeeExperimentAnalysis ( cluster_cols = [ \"cluster\" , \"date\" ], ) pw = PowerAnalysis ( perturbator = perturbator , splitter = sw , analysis = analysis , n_simulations = 50 ) power = pw . power_analysis ( df ) print ( f \" { power = } \" ) add_covariates ( self , df , pre_experiment_df ) \u00b6 Show source code in cluster_experiments/power_analysis.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def add_covariates ( self , df : pd . DataFrame , pre_experiment_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Train model to predict outcome variable (based on pre-experiment data) and add the prediction to the experiment dataframe Args: pre_experiment_df: Dataframe with pre-experiment data. df: Dataframe with outcome and treatment variables. \"\"\" df = df . copy () pre_experiment_df = pre_experiment_df . copy () df_predict , pre_experiment_x , pre_experiment_y = self . _prep_data_cupac ( df = df , pre_experiment_df = pre_experiment_df ) # Fit model self . cupac_model . fit ( pre_experiment_x , pre_experiment_y ) # Predict if isinstance ( self . cupac_model , RegressorMixin ): estimated_target = self . cupac_model . predict ( df_predict ) elif isinstance ( self . cupac_model , ClassifierMixin ): estimated_target = self . cupac_model . predict_proba ( df_predict )[:, 1 ] else : raise ValueError ( \"cupac_model should be an instance of RegressorMixin or ClassifierMixin\" ) # Add cupac outcome name to df df [ self . cupac_outcome_name ] = estimated_target return df Train model to predict outcome variable (based on pre-experiment data) and add the prediction to the experiment dataframe Parameters Name Type Description Default pre_experiment_df DataFrame Dataframe with pre-experiment data. required df DataFrame Dataframe with outcome and treatment variables. required from_config ( config ) (classmethod) \u00b6 Show source code in cluster_experiments/power_analysis.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 @classmethod def from_config ( cls , config : PowerConfig ): \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\" perturbator = cls . _get_mapping_key ( perturbator_mapping , config . perturbator ) . from_config ( config ) splitter = cls . _get_mapping_key ( splitter_mapping , config . splitter ) . from_config ( config ) analysis = cls . _get_mapping_key ( analysis_mapping , config . analysis ) . from_config ( config ) cupac_model = cls . _get_mapping_key ( cupac_model_mapping , config . cupac_model ) . from_config ( config ) return cls ( perturbator = perturbator , splitter = splitter , analysis = analysis , cupac_model = cupac_model , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , n_simulations = config . n_simulations , alpha = config . alpha , ) Constructs PowerAnalysis from PowerConfig from_dict ( config_dict ) (classmethod) \u00b6 Show source code in cluster_experiments/power_analysis.py 240 241 242 243 244 @classmethod def from_dict ( cls , config_dict : dict ): \"\"\"Constructs PowerAnalysis from dictionary\"\"\" config = PowerConfig ( ** config_dict ) return cls . from_config ( config ) Constructs PowerAnalysis from dictionary log_nulls ( self , df ) \u00b6 Show source code in cluster_experiments/power_analysis.py 180 181 182 183 184 185 186 def log_nulls ( self , df : pd . DataFrame ) -> None : \"\"\"Warns about dropping nulls in treatment column\"\"\" n_nulls = len ( df . query ( f \" { self . treatment_col } .isnull()\" )) if n_nulls > 0 : logging . warning ( f \"There are { n_nulls } null values in treatment, dropping them\" ) Warns about dropping nulls in treatment column power_analysis ( self , df , pre_experiment_df = None , verbose = False ) \u00b6 Show source code in cluster_experiments/power_analysis.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def power_analysis ( self , df : pd . DataFrame , pre_experiment_df : Optional [ pd . DataFrame ] = None , verbose : bool = False , ) -> float : \"\"\" Run power analysis by simulation Args: df: Dataframe with outcome and treatment variables. pre_experiment_df: Dataframe with pre-experiment data. \"\"\" df = df . copy () self . _data_checks ( df = df ) if pre_experiment_df is not None and self . is_cupac : df = self . add_covariates ( df , pre_experiment_df ) n_detected_mde = 0 for _ in tqdm ( range ( self . n_simulations ), disable = not verbose ): treatment_df = self . splitter . assign_treatment_df ( df ) self . log_nulls ( treatment_df ) treatment_df = treatment_df . query ( f \" { self . treatment_col } .notnull()\" ) treatment_df = self . perturbator . perturbate ( treatment_df ) p_value = self . analysis . get_pvalue ( treatment_df ) n_detected_mde += p_value < self . alpha return n_detected_mde / self . n_simulations Run power analysis by simulation Parameters Name Type Description Default df DataFrame Dataframe with outcome and treatment variables. required pre_experiment_df Optional[pandas.core.frame.DataFrame] Dataframe with pre-experiment data. None","title":"Power analysis"},{"location":"api/power_analysis.html#from-cluster_experimentspower_analysis-import","text":"","title":"from cluster_experiments.power_analysis import *"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis","text":"Class used to run Power analysis. It does so by running simulations. In each simulation: 1. Assign treatment to dataframe randomly 2. Perturbate dataframe 3. Add pre-experiment data if needed 4. Run analysis Finally it returns the power of the analysis by counting how many times the effect was detected. Parameters Name Type Description Default perturbator Perturbator Perturbator class to perturbate dataframe with treatment assigned. required splitter RandomSplitter RandomSplitter class to randomly assign treatment to dataframe. required analysis ExperimentAnalysis ExperimentAnalysis class to use for analysis. required cupac_model Optional[sklearn.base.BaseEstimator] Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added. None target_col str Name of the column with the outcome variable. 'target' treatment_col str Name of the column with the treatment variable. 'treatment' treatment str value of treatment_col considered to be treatment (not control) 'B' n_simulations int Number of simulations to run. 100 alpha float Significance level. 0.05 features_cupac_model Optional[List[str]] Covariates to be used in cupac model None Usage: from datetime import date import numpy as np import pandas as pd from cluster_experiments.experiment_analysis import GeeExperimentAnalysis from cluster_experiments.perturbator import UniformPerturbator from cluster_experiments.power_analysis import PowerAnalysis from cluster_experiments.random_splitter import SwitchbackSplitter N = 1_000 users = [ f \"User { i } \" for i in range ( 1000 )] clusters = [ f \"Cluster { i } \" for i in range ( 100 )] dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 1 , 32 )] df = pd . DataFrame ( { \"cluster\" : np . random . choice ( clusters , size = N ), \"target\" : np . random . normal ( 0 , 1 , size = N ), \"user\" : np . random . choice ( users , size = N ), \"date\" : np . random . choice ( dates , size = N ), } ) experiment_dates = [ f \" { date ( 2022 , 1 , i ) : %Y-%m-%d } \" for i in range ( 15 , 32 )] sw = SwitchbackSplitter ( clusters = clusters , dates = experiment_dates , ) perturbator = UniformPerturbator ( average_effect = 0.1 , ) analysis = GeeExperimentAnalysis ( cluster_cols = [ \"cluster\" , \"date\" ], ) pw = PowerAnalysis ( perturbator = perturbator , splitter = sw , analysis = analysis , n_simulations = 50 ) power = pw . power_analysis ( df ) print ( f \" { power = } \" )","title":"PowerAnalysis"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.add_covariates","text":"Show source code in cluster_experiments/power_analysis.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def add_covariates ( self , df : pd . DataFrame , pre_experiment_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Train model to predict outcome variable (based on pre-experiment data) and add the prediction to the experiment dataframe Args: pre_experiment_df: Dataframe with pre-experiment data. df: Dataframe with outcome and treatment variables. \"\"\" df = df . copy () pre_experiment_df = pre_experiment_df . copy () df_predict , pre_experiment_x , pre_experiment_y = self . _prep_data_cupac ( df = df , pre_experiment_df = pre_experiment_df ) # Fit model self . cupac_model . fit ( pre_experiment_x , pre_experiment_y ) # Predict if isinstance ( self . cupac_model , RegressorMixin ): estimated_target = self . cupac_model . predict ( df_predict ) elif isinstance ( self . cupac_model , ClassifierMixin ): estimated_target = self . cupac_model . predict_proba ( df_predict )[:, 1 ] else : raise ValueError ( \"cupac_model should be an instance of RegressorMixin or ClassifierMixin\" ) # Add cupac outcome name to df df [ self . cupac_outcome_name ] = estimated_target return df Train model to predict outcome variable (based on pre-experiment data) and add the prediction to the experiment dataframe Parameters Name Type Description Default pre_experiment_df DataFrame Dataframe with pre-experiment data. required df DataFrame Dataframe with outcome and treatment variables. required","title":"add_covariates()"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.from_config","text":"Show source code in cluster_experiments/power_analysis.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 @classmethod def from_config ( cls , config : PowerConfig ): \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\" perturbator = cls . _get_mapping_key ( perturbator_mapping , config . perturbator ) . from_config ( config ) splitter = cls . _get_mapping_key ( splitter_mapping , config . splitter ) . from_config ( config ) analysis = cls . _get_mapping_key ( analysis_mapping , config . analysis ) . from_config ( config ) cupac_model = cls . _get_mapping_key ( cupac_model_mapping , config . cupac_model ) . from_config ( config ) return cls ( perturbator = perturbator , splitter = splitter , analysis = analysis , cupac_model = cupac_model , target_col = config . target_col , treatment_col = config . treatment_col , treatment = config . treatment , n_simulations = config . n_simulations , alpha = config . alpha , ) Constructs PowerAnalysis from PowerConfig","title":"from_config()"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.from_dict","text":"Show source code in cluster_experiments/power_analysis.py 240 241 242 243 244 @classmethod def from_dict ( cls , config_dict : dict ): \"\"\"Constructs PowerAnalysis from dictionary\"\"\" config = PowerConfig ( ** config_dict ) return cls . from_config ( config ) Constructs PowerAnalysis from dictionary","title":"from_dict()"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.log_nulls","text":"Show source code in cluster_experiments/power_analysis.py 180 181 182 183 184 185 186 def log_nulls ( self , df : pd . DataFrame ) -> None : \"\"\"Warns about dropping nulls in treatment column\"\"\" n_nulls = len ( df . query ( f \" { self . treatment_col } .isnull()\" )) if n_nulls > 0 : logging . warning ( f \"There are { n_nulls } null values in treatment, dropping them\" ) Warns about dropping nulls in treatment column","title":"log_nulls()"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.power_analysis","text":"Show source code in cluster_experiments/power_analysis.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def power_analysis ( self , df : pd . DataFrame , pre_experiment_df : Optional [ pd . DataFrame ] = None , verbose : bool = False , ) -> float : \"\"\" Run power analysis by simulation Args: df: Dataframe with outcome and treatment variables. pre_experiment_df: Dataframe with pre-experiment data. \"\"\" df = df . copy () self . _data_checks ( df = df ) if pre_experiment_df is not None and self . is_cupac : df = self . add_covariates ( df , pre_experiment_df ) n_detected_mde = 0 for _ in tqdm ( range ( self . n_simulations ), disable = not verbose ): treatment_df = self . splitter . assign_treatment_df ( df ) self . log_nulls ( treatment_df ) treatment_df = treatment_df . query ( f \" { self . treatment_col } .notnull()\" ) treatment_df = self . perturbator . perturbate ( treatment_df ) p_value = self . analysis . get_pvalue ( treatment_df ) n_detected_mde += p_value < self . alpha return n_detected_mde / self . n_simulations Run power analysis by simulation Parameters Name Type Description Default df DataFrame Dataframe with outcome and treatment variables. required pre_experiment_df Optional[pandas.core.frame.DataFrame] Dataframe with pre-experiment data. None","title":"power_analysis()"},{"location":"api/power_config.html","text":"from cluster_experiments.power_config import * \u00b6 PowerConfig \u00b6 Dataclass to create a power analysis from. Parameters Name Type Description Default splitter str Splitter object to use required perturbator str Perturbator object to use required analysis str ExperimentAnalysis object to use required cupac_model str CUPAC model to use '' n_simulations int number of simulations to run 100 cluster_cols List[str] list of columns to use as clusters required target_col str column to use as target 'target' treatment_col str column to use as treatment 'treatment' treatment str what value of treatment_col should be considered as treatment 'B' covariates Optional[List[str]] list of columns to use as covariates None clusters List[str] list of clusters to use required dates Optional[List[str]] list of dates to use None average_effect float average effect to use in the perturbator 0.0 treatments Optional[List[str]] list of treatments to use None cluster_mapping Optional[Dict[str, str]] mapping of clusters and columns None alpha float alpha value to use in the power analysis 0.05 agg_col str column to use for aggregation in the CUPAC model '' smoothing_factor float smoothing value to use in the CUPAC model 20 features_cupac_model Optional[List[str]] list of features to use in the CUPAC model None Usage: from cluster_experiments.power_config import PowerConfig from cluster_experiments.power_analysis import PowerAnalysis p = PowerConfig ( analysis = \"gee\" , splitter = \"clustered_balance\" , perturbator = \"uniform\" , clusters = [ \"A\" , \"B\" , \"C\" ], cluster_cols = [ \"city\" ], n_simulations = 100 , alpha = 0.05 , ) power_analysis = PowerAnalysis . from_config ( p )","title":"Power config"},{"location":"api/power_config.html#from-cluster_experimentspower_config-import","text":"","title":"from cluster_experiments.power_config import *"},{"location":"api/power_config.html#cluster_experiments.power_config.PowerConfig","text":"Dataclass to create a power analysis from. Parameters Name Type Description Default splitter str Splitter object to use required perturbator str Perturbator object to use required analysis str ExperimentAnalysis object to use required cupac_model str CUPAC model to use '' n_simulations int number of simulations to run 100 cluster_cols List[str] list of columns to use as clusters required target_col str column to use as target 'target' treatment_col str column to use as treatment 'treatment' treatment str what value of treatment_col should be considered as treatment 'B' covariates Optional[List[str]] list of columns to use as covariates None clusters List[str] list of clusters to use required dates Optional[List[str]] list of dates to use None average_effect float average effect to use in the perturbator 0.0 treatments Optional[List[str]] list of treatments to use None cluster_mapping Optional[Dict[str, str]] mapping of clusters and columns None alpha float alpha value to use in the power analysis 0.05 agg_col str column to use for aggregation in the CUPAC model '' smoothing_factor float smoothing value to use in the CUPAC model 20 features_cupac_model Optional[List[str]] list of features to use in the CUPAC model None Usage: from cluster_experiments.power_config import PowerConfig from cluster_experiments.power_analysis import PowerAnalysis p = PowerConfig ( analysis = \"gee\" , splitter = \"clustered_balance\" , perturbator = \"uniform\" , clusters = [ \"A\" , \"B\" , \"C\" ], cluster_cols = [ \"city\" ], n_simulations = 100 , alpha = 0.05 , ) power_analysis = PowerAnalysis . from_config ( p )","title":"PowerConfig"},{"location":"api/random_splitter.html","text":"from cluster_experiments.random_splitter import * \u00b6 BalancedClusteredSplitter \u00b6 Like ClusteredSplitter, but ensures that treatments are balanced among clusters. That is, if we have 25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B. get_balanced_sample ( self , clusters_per_treatment , remainder_clusters ) \u00b6 Show source code in cluster_experiments/random_splitter.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_balanced_sample ( self , clusters_per_treatment : int , remainder_clusters : int ) -> List [ str ]: \"\"\"Given the number of clusters per treatment and the remainder clusters (results of the integer division between the number of clusters and the number of treatments), obtain, in the most balanced way, a list of treatments such that the difference between the number of clusters per treatment is minimal among clusters\"\"\" remainder_treatments = random . sample ( self . treatments , k = remainder_clusters ) sampled_treatments = [] for treatment in self . treatments : sampled_treatments . extend ([ treatment ] * clusters_per_treatment ) sampled_treatments . extend ( remainder_treatments ) random . shuffle ( sampled_treatments ) return sampled_treatments Given the number of clusters per treatment and the remainder clusters (results of the integer division between the number of clusters and the number of treatments), obtain, in the most balanced way, a list of treatments such that the difference between the number of clusters per treatment is minimal among clusters sample_treatment ( self , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 213 214 215 216 217 218 219 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Randomly assign a treatment to a cluster\"\"\" if len ( self . clusters ) < len ( self . treatments ): raise ValueError ( \"There are more treatments than clusters\" ) clusters_per_treatment = len ( self . clusters ) // len ( self . treatments ) remainder_clusters = len ( self . clusters ) % len ( self . treatments ) return self . get_balanced_sample ( clusters_per_treatment , remainder_clusters ) Randomly assign a treatment to a cluster BalancedSwitchbackSplitter \u00b6 Like SwitchbackSplitter, but ensures that treatments are balanced among clusters. Usage: import pandas as pd from cluster_experiments.random_splitter import BalancedSwitchbackSplitter splitter = BalancedSwitchbackSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], dates = [ \"2020-01-01\" , \"2020-01-02\" ], cluster_mapping = { \"cluster\" : \"city\" , \"date\" : \"date\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ], \"date\" : [ \"2020-01-01\" , \"2020-01-02\" , \"2020-01-01\" ]}) df = splitter . assign_treatment_df ( df ) print ( df ) sample_treatment ( self , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 260 261 262 263 264 265 266 267 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: if len ( self . clusters ) * len ( self . dates ) < len ( self . treatments ): raise ValueError ( \"There are more treatments than clusters and dates\" ) total_switches = len ( self . clusters ) * len ( self . dates ) clusters_per_treatment = total_switches // len ( self . treatments ) remainder_clusters = total_switches % len ( self . treatments ) return self . get_balanced_sample ( clusters_per_treatment , remainder_clusters ) Randomly assign a treatment to a cluster ClusteredSplitter \u00b6 Splits randomly using clusters Usage: import pandas as pd from cluster_experiments.random_splitter import ClusteredSplitter splitter = ClusteredSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], cluster_mapping = { \"cluster\" : \"city\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ]}) df = splitter . assign_treatment_df ( df ) print ( df ) sample_treatment ( self , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 133 134 135 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Choose randomly a treatment for each cluster\"\"\" return random . choices ( self . treatments , k = len ( self . clusters )) Choose randomly a treatment for each cluster treatment_assignment ( self , sampled_treatments ) \u00b6 Show source code in cluster_experiments/random_splitter.py 124 125 126 127 128 129 130 131 def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\"Assign each sampled treatment to a cluster\"\"\" return [ { \"treatment\" : treatment , \"cluster\" : cluster } for treatment , cluster in zip ( sampled_treatments , self . clusters ) ] Assign each sampled treatment to a cluster RandomSplitter \u00b6 Abstract class to split instances in a switchback or clustered way. It can be used to create a calendar/split of clusters or to run a power analysis. In order to create your own RandomSplitter, you need to write two methods: * treatment_assignment: If you are deriving from clustered or switchback splitters, no need for this. The goal of this is, given the output of sample_treatment, prepare such that it can be added to the dataframe by building a list of dictionaries with clusters and treatments. * sample_treatment: This is what needs to be implemented. It should return a list of same length as the number of clusters, with the treatment received to each cluster. Parameters Name Type Description Default clusters List[str] list of clusters to split required treatments Optional[List[str]] list of treatments None dates Optional[List[str]] list of dates (switches) None cluster_mapping Optional[Dict[str, str]] dictionary to map the keys cluster and date to the actual names of the columns of the dataframe. For clustered splitter, cluster_mapping could be {\"cluster\": \"city\"}. for SwitchbackSplitter, cluster_mapping could be None assign_treatment_df ( self , df , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def assign_treatment_df ( self , df : pd . DataFrame , * args , ** kwargs , ) -> pd . DataFrame : \"\"\" Takes a df, randomizes treatments and adds the treatment column to the dataframe Arguments: df: dataframe to assign treatments to args: arguments to pass to sample_treatment kwargs: keyword arguments to pass to sample_treatment \"\"\" df = df . copy () sampled_treatments = self . sample_treatment ( * args , ** kwargs ) treatments_df = pd . DataFrame ( self . treatment_assignment ( sampled_treatments ) ) . rename ( columns = self . cluster_mapping ) join_columns = list ( self . cluster_mapping . values ()) return df . merge ( treatments_df , how = \"left\" , on = join_columns ) Takes a df, randomizes treatments and adds the treatment column to the dataframe Parameters Name Type Description Default df DataFrame dataframe to assign treatments to required *args arguments to pass to sample_treatment () **kwargs keyword arguments to pass to sample_treatment {} from_config ( config ) (classmethod) \u00b6 Show source code in cluster_experiments/random_splitter.py 79 80 81 82 83 84 85 86 87 @classmethod def from_config ( cls , config ): \"\"\"Creates a RandomSplitter from a PowerConfig\"\"\" return cls ( clusters = config . clusters , treatments = config . treatments , dates = config . dates , cluster_mapping = config . cluster_mapping , ) Creates a RandomSplitter from a PowerConfig sample_treatment ( self , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 50 51 52 53 54 55 @abstractmethod def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\" Randomly samples treatments for each cluster. \"\"\" pass Randomly samples treatments for each cluster. treatment_assignment ( self , sampled_treatments ) \u00b6 Show source code in cluster_experiments/random_splitter.py 40 41 42 43 44 45 46 47 48 @abstractmethod def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\" Prepares the data of the treatment assignment for the dataframe. It should take as input some list of treatments [\"A\", \"B\", \"B\", \"A\"] and return a list of dictionaries, where each element has information about the cluster and treatment, like {\"cluster\": \"Cluster 1\", \"treatment\": \"A\"}. \"\"\" pass Prepares the data of the treatment assignment for the dataframe. It should take as input some list of treatments [\"A\", \"B\", \"B\", \"A\"] and return a list of dictionaries, where each element has information about the cluster and treatment, like {\"cluster\": \"Cluster 1\", \"treatment\": \"A\"}. SwitchbackSplitter \u00b6 Splits randomly using clusters and dates Usage: import pandas as pd from cluster_experiments.random_splitter import SwitchbackSplitter splitter = SwitchbackSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], dates = [ \"2020-01-01\" , \"2020-01-02\" ], cluster_mapping = { \"cluster\" : \"city\" , \"date\" : \"date\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ], \"date\" : [ \"2020-01-01\" , \"2020-01-02\" , \"2020-01-01\" ]}) df = splitter . assign_treatment_df ( df ) print ( df ) sample_treatment ( self , * args , ** kwargs ) \u00b6 Show source code in cluster_experiments/random_splitter.py 188 189 190 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Randomly assign a treatment to a cluster\"\"\" return random . choices ( self . treatments , k = len ( self . clusters ) * len ( self . dates )) Randomly assign a treatment to a cluster treatment_assignment ( self , sampled_treatments ) \u00b6 Show source code in cluster_experiments/random_splitter.py 177 178 179 180 181 182 183 184 185 186 def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\"For each date, we get, on each cluster, the treatment assigned to it\"\"\" sampled_treatments = sampled_treatments . copy () output = [] for date , cluster in product ( self . dates , self . clusters ): treatment = sampled_treatments . pop ( 0 ) output . append ({ \"date\" : date , \"cluster\" : cluster , \"treatment\" : treatment }) return output For each date, we get, on each cluster, the treatment assigned to it","title":"Splitter"},{"location":"api/random_splitter.html#from-cluster_experimentsrandom_splitter-import","text":"","title":"from cluster_experiments.random_splitter import *"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedClusteredSplitter","text":"Like ClusteredSplitter, but ensures that treatments are balanced among clusters. That is, if we have 25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B.","title":"BalancedClusteredSplitter"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedClusteredSplitter.get_balanced_sample","text":"Show source code in cluster_experiments/random_splitter.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_balanced_sample ( self , clusters_per_treatment : int , remainder_clusters : int ) -> List [ str ]: \"\"\"Given the number of clusters per treatment and the remainder clusters (results of the integer division between the number of clusters and the number of treatments), obtain, in the most balanced way, a list of treatments such that the difference between the number of clusters per treatment is minimal among clusters\"\"\" remainder_treatments = random . sample ( self . treatments , k = remainder_clusters ) sampled_treatments = [] for treatment in self . treatments : sampled_treatments . extend ([ treatment ] * clusters_per_treatment ) sampled_treatments . extend ( remainder_treatments ) random . shuffle ( sampled_treatments ) return sampled_treatments Given the number of clusters per treatment and the remainder clusters (results of the integer division between the number of clusters and the number of treatments), obtain, in the most balanced way, a list of treatments such that the difference between the number of clusters per treatment is minimal among clusters","title":"get_balanced_sample()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedClusteredSplitter.sample_treatment","text":"Show source code in cluster_experiments/random_splitter.py 213 214 215 216 217 218 219 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Randomly assign a treatment to a cluster\"\"\" if len ( self . clusters ) < len ( self . treatments ): raise ValueError ( \"There are more treatments than clusters\" ) clusters_per_treatment = len ( self . clusters ) // len ( self . treatments ) remainder_clusters = len ( self . clusters ) % len ( self . treatments ) return self . get_balanced_sample ( clusters_per_treatment , remainder_clusters ) Randomly assign a treatment to a cluster","title":"sample_treatment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedSwitchbackSplitter","text":"Like SwitchbackSplitter, but ensures that treatments are balanced among clusters. Usage: import pandas as pd from cluster_experiments.random_splitter import BalancedSwitchbackSplitter splitter = BalancedSwitchbackSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], dates = [ \"2020-01-01\" , \"2020-01-02\" ], cluster_mapping = { \"cluster\" : \"city\" , \"date\" : \"date\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ], \"date\" : [ \"2020-01-01\" , \"2020-01-02\" , \"2020-01-01\" ]}) df = splitter . assign_treatment_df ( df ) print ( df )","title":"BalancedSwitchbackSplitter"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedSwitchbackSplitter.sample_treatment","text":"Show source code in cluster_experiments/random_splitter.py 260 261 262 263 264 265 266 267 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: if len ( self . clusters ) * len ( self . dates ) < len ( self . treatments ): raise ValueError ( \"There are more treatments than clusters and dates\" ) total_switches = len ( self . clusters ) * len ( self . dates ) clusters_per_treatment = total_switches // len ( self . treatments ) remainder_clusters = total_switches % len ( self . treatments ) return self . get_balanced_sample ( clusters_per_treatment , remainder_clusters ) Randomly assign a treatment to a cluster","title":"sample_treatment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter","text":"Splits randomly using clusters Usage: import pandas as pd from cluster_experiments.random_splitter import ClusteredSplitter splitter = ClusteredSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], cluster_mapping = { \"cluster\" : \"city\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ]}) df = splitter . assign_treatment_df ( df ) print ( df )","title":"ClusteredSplitter"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter.sample_treatment","text":"Show source code in cluster_experiments/random_splitter.py 133 134 135 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Choose randomly a treatment for each cluster\"\"\" return random . choices ( self . treatments , k = len ( self . clusters )) Choose randomly a treatment for each cluster","title":"sample_treatment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter.treatment_assignment","text":"Show source code in cluster_experiments/random_splitter.py 124 125 126 127 128 129 130 131 def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\"Assign each sampled treatment to a cluster\"\"\" return [ { \"treatment\" : treatment , \"cluster\" : cluster } for treatment , cluster in zip ( sampled_treatments , self . clusters ) ] Assign each sampled treatment to a cluster","title":"treatment_assignment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter","text":"Abstract class to split instances in a switchback or clustered way. It can be used to create a calendar/split of clusters or to run a power analysis. In order to create your own RandomSplitter, you need to write two methods: * treatment_assignment: If you are deriving from clustered or switchback splitters, no need for this. The goal of this is, given the output of sample_treatment, prepare such that it can be added to the dataframe by building a list of dictionaries with clusters and treatments. * sample_treatment: This is what needs to be implemented. It should return a list of same length as the number of clusters, with the treatment received to each cluster. Parameters Name Type Description Default clusters List[str] list of clusters to split required treatments Optional[List[str]] list of treatments None dates Optional[List[str]] list of dates (switches) None cluster_mapping Optional[Dict[str, str]] dictionary to map the keys cluster and date to the actual names of the columns of the dataframe. For clustered splitter, cluster_mapping could be {\"cluster\": \"city\"}. for SwitchbackSplitter, cluster_mapping could be None","title":"RandomSplitter"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.assign_treatment_df","text":"Show source code in cluster_experiments/random_splitter.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def assign_treatment_df ( self , df : pd . DataFrame , * args , ** kwargs , ) -> pd . DataFrame : \"\"\" Takes a df, randomizes treatments and adds the treatment column to the dataframe Arguments: df: dataframe to assign treatments to args: arguments to pass to sample_treatment kwargs: keyword arguments to pass to sample_treatment \"\"\" df = df . copy () sampled_treatments = self . sample_treatment ( * args , ** kwargs ) treatments_df = pd . DataFrame ( self . treatment_assignment ( sampled_treatments ) ) . rename ( columns = self . cluster_mapping ) join_columns = list ( self . cluster_mapping . values ()) return df . merge ( treatments_df , how = \"left\" , on = join_columns ) Takes a df, randomizes treatments and adds the treatment column to the dataframe Parameters Name Type Description Default df DataFrame dataframe to assign treatments to required *args arguments to pass to sample_treatment () **kwargs keyword arguments to pass to sample_treatment {}","title":"assign_treatment_df()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.from_config","text":"Show source code in cluster_experiments/random_splitter.py 79 80 81 82 83 84 85 86 87 @classmethod def from_config ( cls , config ): \"\"\"Creates a RandomSplitter from a PowerConfig\"\"\" return cls ( clusters = config . clusters , treatments = config . treatments , dates = config . dates , cluster_mapping = config . cluster_mapping , ) Creates a RandomSplitter from a PowerConfig","title":"from_config()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.sample_treatment","text":"Show source code in cluster_experiments/random_splitter.py 50 51 52 53 54 55 @abstractmethod def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\" Randomly samples treatments for each cluster. \"\"\" pass Randomly samples treatments for each cluster.","title":"sample_treatment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.treatment_assignment","text":"Show source code in cluster_experiments/random_splitter.py 40 41 42 43 44 45 46 47 48 @abstractmethod def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\" Prepares the data of the treatment assignment for the dataframe. It should take as input some list of treatments [\"A\", \"B\", \"B\", \"A\"] and return a list of dictionaries, where each element has information about the cluster and treatment, like {\"cluster\": \"Cluster 1\", \"treatment\": \"A\"}. \"\"\" pass Prepares the data of the treatment assignment for the dataframe. It should take as input some list of treatments [\"A\", \"B\", \"B\", \"A\"] and return a list of dictionaries, where each element has information about the cluster and treatment, like {\"cluster\": \"Cluster 1\", \"treatment\": \"A\"}.","title":"treatment_assignment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter","text":"Splits randomly using clusters and dates Usage: import pandas as pd from cluster_experiments.random_splitter import SwitchbackSplitter splitter = SwitchbackSplitter ( clusters = [ \"A\" , \"B\" , \"C\" ], treatments = [ \"A\" , \"B\" ], dates = [ \"2020-01-01\" , \"2020-01-02\" ], cluster_mapping = { \"cluster\" : \"city\" , \"date\" : \"date\" }, ) df = pd . DataFrame ({ \"city\" : [ \"A\" , \"B\" , \"C\" ], \"date\" : [ \"2020-01-01\" , \"2020-01-02\" , \"2020-01-01\" ]}) df = splitter . assign_treatment_df ( df ) print ( df )","title":"SwitchbackSplitter"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter.sample_treatment","text":"Show source code in cluster_experiments/random_splitter.py 188 189 190 def sample_treatment ( self , * args , ** kwargs ) -> List [ str ]: \"\"\"Randomly assign a treatment to a cluster\"\"\" return random . choices ( self . treatments , k = len ( self . clusters ) * len ( self . dates )) Randomly assign a treatment to a cluster","title":"sample_treatment()"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter.treatment_assignment","text":"Show source code in cluster_experiments/random_splitter.py 177 178 179 180 181 182 183 184 185 186 def treatment_assignment ( self , sampled_treatments : List [ str ] ) -> List [ Dict [ str , str ]]: \"\"\"For each date, we get, on each cluster, the treatment assigned to it\"\"\" sampled_treatments = sampled_treatments . copy () output = [] for date , cluster in product ( self . dates , self . clusters ): treatment = sampled_treatments . pop ( 0 ) output . append ({ \"date\" : date , \"cluster\" : cluster , \"treatment\" : treatment }) return output For each date, we get, on each cluster, the treatment assigned to it","title":"treatment_assignment()"}]}